{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdBCTASP+m+ocRqj6W54UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanaelyao/CISC-474-Final-Project/blob/main/2048_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avx1rX_eP24D",
        "outputId": "85a6ea87-aaa5-4cac-ab07-9946a255bf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "!pip install git+https://github.com/rgal/gym-2048\n",
        "import gym_2048"
      ],
      "metadata": {
        "id": "gsBG6Y5ERU95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25321b3-c6ae-4987-e2a1-9e8d5f2f04db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rgal/gym-2048\n",
            "  Cloning https://github.com/rgal/gym-2048 to /tmp/pip-req-build-zz6lr637\n",
            "  Running command git clone -q https://github.com/rgal/gym-2048 /tmp/pip-req-build-zz6lr637\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from gym-2048==0.0.5) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-2048==0.0.5) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->gym-2048==0.0.5) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-2048==0.0.5) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-2048==0.0.5) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->gym-2048==0.0.5) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('2048-v0')\n",
        "env.seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WvdNewDIXZ0",
        "outputId": "4c7990ae-bdf2-4abd-ce86-09a1b2c29ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: int64. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def DeepQ(input_shape, num_actions):\n",
        "  input = layers.Input(shape=input_shape)\n",
        "  layer = layers.Dense(units=20, activation=\"relu\", name=\"hidden1\")(layers.Flatten()(input))\n",
        "  layer = layers.Dense(units=20, activation=\"relu\", name=\"hidden2\")(layer)\n",
        "  #layer = layers.Dense(units=256, activation=\"relu\", name=\"hidden3\")(layer)\n",
        "  output = layers.Dense(units=num_actions, activation=\"linear\", name=\"output\")(layer)\n",
        "\n",
        "  return keras.Model(inputs=input, outputs=output)"
      ],
      "metadata": {
        "id": "19R-5b3CSCKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEEP Q-LEARNING v2.0\n",
        "# Parameters\n",
        "episodes = 5000\n",
        "discount = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "#epsilon_max = 1.0\n",
        "#epsilon_decrement = epsilon_max - epsilon_min / (episodes // 4)\n",
        "epsilon_decrement = 0.995\n",
        "batch_size = 32  # Batch size to be taken from replay memory\n",
        "\n",
        "replay = []  # Replay memory, stores experiences as e=(s, a, r, s', done)\n",
        "max_memory_length = 2000  # Max size of replay memory\n",
        "update_t_network = 500  # Number of time steps before updating target network\n",
        "warm_up = 1000  # Number of steps to run before beginning to train the policy network\n",
        "num_actions = env.action_space.n  # Number of possible actions\n",
        "input_shape = env.observation_space.shape  # Input shape (4, 4, 16)\n",
        "\n",
        "ep_reward_history = [0]\n",
        "high_score = 0\n",
        "total_steps = 0  \n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # Optimizer\n",
        "loss=\"mse\"  # Loss function\n",
        "p_network = DeepQ(input_shape, num_actions)  # Initialize policy network\n",
        "p_network.compile(optimizer=optimizer, loss=loss)\n",
        "t_network = DeepQ(input_shape, num_actions)  # Initialize target network\n",
        "t_network.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "p_network.summary()\n",
        "\n",
        "# Train\n",
        "for ep in range(episodes):\n",
        "  s = np.array(env.reset())\n",
        "  ep_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    total_steps += 1\n",
        "    # Select action with e-greedy\n",
        "    if np.random.random() < epsilon:  # Random choice\n",
        "      a = np.random.choice(num_actions)\n",
        "    else:  # Greedy choice\n",
        "      a = p_network.predict(np.reshape(s, (1,4,4,16)), verbose=0)\n",
        "      a = np.argmax(a[0])\n",
        "    \n",
        "    # Epsilon decay\n",
        "    # epsilon = max(epsilon - epsilon_decrement, epsilon_min)\n",
        "    epsilon = max(epsilon * epsilon_decrement, epsilon_min)\n",
        "    \n",
        "    # Execute action and observe s', reward, and done\n",
        "    s_prime, reward, done, info = env.step(a)\n",
        "    s_prime = np.array(s_prime)\n",
        "    ep_reward += reward\n",
        "    \n",
        "    # Store experience in replay memory up to the last N number of experiences\n",
        "    replay.append((s,a,reward,s_prime,done))\n",
        "    s = s_prime\n",
        "    if len(replay) > max_memory_length:\n",
        "      del replay[0]\n",
        "    \n",
        "    if len(replay) > batch_size:\n",
        "      # Sample random batch from replay memory\n",
        "      batch = random.sample(replay, batch_size)\n",
        "      batch_s = np.array([e[0] for e in batch])\n",
        "      batch_a = np.array([e[1] for e in batch])\n",
        "      batch_rewards = np.array([e[2] for e in batch])\n",
        "      batch_s_prime = np.array([e[3] for e in batch])\n",
        "\n",
        "      targets = batch_rewards\n",
        "      if not done:\n",
        "        targets = batch_rewards + discount * np.amax(t_network.predict(batch_s_prime, verbose=0), axis=1)\n",
        "      p_network.fit(batch_s, targets, verbose=0)\n",
        "\n",
        "    # After x time steps, update target network with policy network weights\n",
        "    if total_steps % update_t_network == 0:\n",
        "      t_network.set_weights(p_network.get_weights())\n",
        "    \n",
        "    # Track highest score reach by any episode\n",
        "    if done:\n",
        "      print(\"Episode: {}/{}, Reward: {}, High score: {}, e: {:.2}\".format(ep, episodes, ep_reward, info['highest'], epsilon))\n",
        "      if info['highest'] > high_score:\n",
        "        high_score = info['highest']\n",
        "  \n",
        "  # Track reward achieved by each episode\n",
        "  ep_reward_history.append(ep_reward)\n",
        "\n",
        "print(max(high_score))\n",
        "plt.plot(ep_reward_history)\n",
        "plt.title(\"Deep Q-Learning 2048 Rewards\", fontsize='large')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O7GFYp19cHyM",
        "outputId": "a38d5b3c-2443-446f-b88a-028873932ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 4, 4, 16)]        0         \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " hidden1 (Dense)             (None, 20)                5140      \n",
            "                                                                 \n",
            " hidden2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " output (Dense)              (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,644\n",
            "Trainable params: 5,644\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 0/10000, Reward: 108.0, High score: 16, e: 0.99\n",
            "Episode: 1/10000, Reward: 8.0, High score: 4, e: 0.99\n",
            "Episode: 2/10000, Reward: 24.0, High score: 8, e: 0.98\n",
            "Episode: 3/10000, Reward: 16.0, High score: 8, e: 0.98\n",
            "Episode: 4/10000, Reward: 8.0, High score: 4, e: 0.98\n",
            "Episode: 5/10000, Reward: 4.0, High score: 4, e: 0.97\n",
            "Episode: 6/10000, Reward: 76.0, High score: 16, e: 0.96\n",
            "Episode: 7/10000, Reward: 20.0, High score: 8, e: 0.96\n",
            "Episode: 8/10000, Reward: 4.0, High score: 4, e: 0.96\n",
            "Episode: 9/10000, Reward: 96.0, High score: 16, e: 0.95\n",
            "Episode: 10/10000, Reward: 16.0, High score: 8, e: 0.94\n",
            "Episode: 11/10000, Reward: 64.0, High score: 16, e: 0.94\n",
            "Episode: 12/10000, Reward: 68.0, High score: 16, e: 0.93\n",
            "Episode: 13/10000, Reward: 144.0, High score: 32, e: 0.92\n",
            "Episode: 14/10000, Reward: 56.0, High score: 16, e: 0.91\n",
            "Episode: 15/10000, Reward: 20.0, High score: 8, e: 0.91\n",
            "Episode: 16/10000, Reward: 4.0, High score: 4, e: 0.91\n",
            "Episode: 17/10000, Reward: 28.0, High score: 8, e: 0.9\n",
            "Episode: 18/10000, Reward: 84.0, High score: 16, e: 0.89\n",
            "Episode: 19/10000, Reward: 12.0, High score: 4, e: 0.89\n",
            "Episode: 20/10000, Reward: 72.0, High score: 16, e: 0.87\n",
            "Episode: 21/10000, Reward: 208.0, High score: 32, e: 0.86\n",
            "Episode: 22/10000, Reward: 4.0, High score: 4, e: 0.86\n",
            "Episode: 23/10000, Reward: 132.0, High score: 32, e: 0.85\n",
            "Episode: 24/10000, Reward: 16.0, High score: 8, e: 0.84\n",
            "Episode: 25/10000, Reward: 16.0, High score: 8, e: 0.84\n",
            "Episode: 26/10000, Reward: 60.0, High score: 16, e: 0.84\n",
            "Episode: 27/10000, Reward: 112.0, High score: 16, e: 0.82\n",
            "Episode: 28/10000, Reward: 32.0, High score: 8, e: 0.82\n",
            "Episode: 29/10000, Reward: 24.0, High score: 8, e: 0.82\n",
            "Episode: 30/10000, Reward: 64.0, High score: 16, e: 0.81\n",
            "Episode: 31/10000, Reward: 20.0, High score: 8, e: 0.8\n",
            "Episode: 32/10000, Reward: 8.0, High score: 4, e: 0.8\n",
            "Episode: 33/10000, Reward: 44.0, High score: 8, e: 0.8\n",
            "Episode: 34/10000, Reward: 0.0, High score: 2, e: 0.8\n",
            "Episode: 35/10000, Reward: 8.0, High score: 4, e: 0.79\n",
            "Episode: 36/10000, Reward: 12.0, High score: 8, e: 0.79\n",
            "Episode: 37/10000, Reward: 152.0, High score: 32, e: 0.78\n",
            "Episode: 38/10000, Reward: 120.0, High score: 16, e: 0.77\n",
            "Episode: 39/10000, Reward: 152.0, High score: 32, e: 0.76\n",
            "Episode: 40/10000, Reward: 4.0, High score: 4, e: 0.76\n",
            "Episode: 41/10000, Reward: 84.0, High score: 16, e: 0.75\n",
            "Episode: 42/10000, Reward: 84.0, High score: 16, e: 0.74\n",
            "Episode: 43/10000, Reward: 36.0, High score: 8, e: 0.74\n",
            "Episode: 44/10000, Reward: 4.0, High score: 4, e: 0.74\n",
            "Episode: 45/10000, Reward: 4.0, High score: 4, e: 0.73\n",
            "Episode: 46/10000, Reward: 40.0, High score: 8, e: 0.73\n",
            "Episode: 47/10000, Reward: 4.0, High score: 4, e: 0.73\n",
            "Episode: 48/10000, Reward: 80.0, High score: 16, e: 0.72\n",
            "Episode: 49/10000, Reward: 4.0, High score: 4, e: 0.72\n",
            "Episode: 50/10000, Reward: 0.0, High score: 4, e: 0.72\n",
            "Episode: 51/10000, Reward: 4.0, High score: 4, e: 0.72\n",
            "Episode: 52/10000, Reward: 100.0, High score: 16, e: 0.71\n",
            "Episode: 53/10000, Reward: 0.0, High score: 2, e: 0.71\n",
            "Episode: 54/10000, Reward: 4.0, High score: 4, e: 0.71\n",
            "Episode: 55/10000, Reward: 24.0, High score: 8, e: 0.7\n",
            "Episode: 56/10000, Reward: 24.0, High score: 8, e: 0.7\n",
            "Episode: 57/10000, Reward: 16.0, High score: 8, e: 0.7\n",
            "Episode: 58/10000, Reward: 0.0, High score: 2, e: 0.7\n",
            "Episode: 59/10000, Reward: 4.0, High score: 4, e: 0.7\n",
            "Episode: 60/10000, Reward: 0.0, High score: 2, e: 0.7\n",
            "Episode: 61/10000, Reward: 8.0, High score: 4, e: 0.69\n",
            "Episode: 62/10000, Reward: 16.0, High score: 8, e: 0.69\n",
            "Episode: 63/10000, Reward: 88.0, High score: 16, e: 0.68\n",
            "Episode: 64/10000, Reward: 32.0, High score: 8, e: 0.68\n",
            "Episode: 65/10000, Reward: 16.0, High score: 8, e: 0.68\n",
            "Episode: 66/10000, Reward: 28.0, High score: 8, e: 0.67\n",
            "Episode: 67/10000, Reward: 4.0, High score: 4, e: 0.67\n",
            "Episode: 68/10000, Reward: 24.0, High score: 8, e: 0.67\n",
            "Episode: 69/10000, Reward: 140.0, High score: 32, e: 0.66\n",
            "Episode: 70/10000, Reward: 64.0, High score: 16, e: 0.66\n",
            "Episode: 71/10000, Reward: 48.0, High score: 8, e: 0.65\n",
            "Episode: 72/10000, Reward: 0.0, High score: 2, e: 0.65\n",
            "Episode: 73/10000, Reward: 56.0, High score: 16, e: 0.65\n",
            "Episode: 74/10000, Reward: 16.0, High score: 8, e: 0.64\n",
            "Episode: 75/10000, Reward: 20.0, High score: 8, e: 0.64\n",
            "Episode: 76/10000, Reward: 284.0, High score: 32, e: 0.63\n",
            "Episode: 77/10000, Reward: 64.0, High score: 16, e: 0.62\n",
            "Episode: 78/10000, Reward: 168.0, High score: 32, e: 0.61\n",
            "Episode: 79/10000, Reward: 60.0, High score: 8, e: 0.61\n",
            "Episode: 80/10000, Reward: 0.0, High score: 4, e: 0.61\n",
            "Episode: 81/10000, Reward: 32.0, High score: 8, e: 0.6\n",
            "Episode: 82/10000, Reward: 4.0, High score: 4, e: 0.6\n",
            "Episode: 83/10000, Reward: 64.0, High score: 16, e: 0.6\n",
            "Episode: 84/10000, Reward: 4.0, High score: 4, e: 0.59\n",
            "Episode: 85/10000, Reward: 28.0, High score: 8, e: 0.59\n",
            "Episode: 86/10000, Reward: 0.0, High score: 2, e: 0.59\n",
            "Episode: 87/10000, Reward: 32.0, High score: 8, e: 0.59\n",
            "Episode: 88/10000, Reward: 88.0, High score: 16, e: 0.58\n",
            "Episode: 89/10000, Reward: 100.0, High score: 16, e: 0.57\n",
            "Episode: 90/10000, Reward: 8.0, High score: 4, e: 0.57\n",
            "Episode: 91/10000, Reward: 0.0, High score: 4, e: 0.57\n",
            "Episode: 92/10000, Reward: 84.0, High score: 16, e: 0.57\n",
            "Episode: 93/10000, Reward: 84.0, High score: 16, e: 0.56\n",
            "Episode: 94/10000, Reward: 0.0, High score: 4, e: 0.56\n",
            "Episode: 95/10000, Reward: 0.0, High score: 4, e: 0.56\n",
            "Episode: 96/10000, Reward: 0.0, High score: 2, e: 0.56\n",
            "Episode: 97/10000, Reward: 64.0, High score: 16, e: 0.55\n",
            "Episode: 98/10000, Reward: 16.0, High score: 8, e: 0.55\n",
            "Episode: 99/10000, Reward: 16.0, High score: 8, e: 0.55\n",
            "Episode: 100/10000, Reward: 48.0, High score: 8, e: 0.54\n",
            "Episode: 101/10000, Reward: 80.0, High score: 16, e: 0.54\n",
            "Episode: 102/10000, Reward: 56.0, High score: 16, e: 0.53\n",
            "Episode: 103/10000, Reward: 40.0, High score: 8, e: 0.53\n",
            "Episode: 104/10000, Reward: 64.0, High score: 16, e: 0.53\n",
            "Episode: 105/10000, Reward: 88.0, High score: 16, e: 0.52\n",
            "Episode: 106/10000, Reward: 176.0, High score: 32, e: 0.51\n",
            "Episode: 107/10000, Reward: 68.0, High score: 16, e: 0.51\n",
            "Episode: 108/10000, Reward: 4.0, High score: 4, e: 0.51\n",
            "Episode: 109/10000, Reward: 36.0, High score: 8, e: 0.5\n",
            "Episode: 110/10000, Reward: 4.0, High score: 4, e: 0.5\n",
            "Episode: 111/10000, Reward: 76.0, High score: 16, e: 0.5\n",
            "Episode: 112/10000, Reward: 24.0, High score: 8, e: 0.5\n",
            "Episode: 113/10000, Reward: 120.0, High score: 16, e: 0.49\n",
            "Episode: 114/10000, Reward: 0.0, High score: 4, e: 0.49\n",
            "Episode: 115/10000, Reward: 20.0, High score: 8, e: 0.49\n",
            "Episode: 116/10000, Reward: 64.0, High score: 16, e: 0.48\n",
            "Episode: 117/10000, Reward: 12.0, High score: 4, e: 0.48\n",
            "Episode: 118/10000, Reward: 4.0, High score: 4, e: 0.48\n",
            "Episode: 119/10000, Reward: 108.0, High score: 16, e: 0.47\n",
            "Episode: 120/10000, Reward: 80.0, High score: 16, e: 0.47\n",
            "Episode: 121/10000, Reward: 48.0, High score: 8, e: 0.46\n",
            "Episode: 122/10000, Reward: 16.0, High score: 8, e: 0.46\n",
            "Episode: 123/10000, Reward: 32.0, High score: 8, e: 0.46\n",
            "Episode: 124/10000, Reward: 0.0, High score: 4, e: 0.46\n",
            "Episode: 125/10000, Reward: 12.0, High score: 4, e: 0.46\n",
            "Episode: 126/10000, Reward: 36.0, High score: 8, e: 0.46\n",
            "Episode: 127/10000, Reward: 88.0, High score: 16, e: 0.45\n",
            "Episode: 128/10000, Reward: 84.0, High score: 16, e: 0.44\n",
            "Episode: 129/10000, Reward: 12.0, High score: 8, e: 0.44\n",
            "Episode: 130/10000, Reward: 56.0, High score: 8, e: 0.44\n",
            "Episode: 131/10000, Reward: 60.0, High score: 16, e: 0.43\n",
            "Episode: 132/10000, Reward: 64.0, High score: 16, e: 0.43\n",
            "Episode: 133/10000, Reward: 20.0, High score: 8, e: 0.43\n",
            "Episode: 134/10000, Reward: 56.0, High score: 8, e: 0.43\n",
            "Episode: 135/10000, Reward: 56.0, High score: 16, e: 0.42\n",
            "Episode: 136/10000, Reward: 408.0, High score: 64, e: 0.41\n",
            "Episode: 137/10000, Reward: 40.0, High score: 8, e: 0.41\n",
            "Episode: 138/10000, Reward: 220.0, High score: 32, e: 0.4\n",
            "Episode: 139/10000, Reward: 8.0, High score: 4, e: 0.4\n",
            "Episode: 140/10000, Reward: 20.0, High score: 4, e: 0.4\n",
            "Episode: 141/10000, Reward: 92.0, High score: 16, e: 0.39\n",
            "Episode: 142/10000, Reward: 68.0, High score: 16, e: 0.39\n",
            "Episode: 143/10000, Reward: 56.0, High score: 16, e: 0.39\n",
            "Episode: 144/10000, Reward: 28.0, High score: 8, e: 0.38\n",
            "Episode: 145/10000, Reward: 164.0, High score: 16, e: 0.38\n",
            "Episode: 146/10000, Reward: 0.0, High score: 4, e: 0.38\n",
            "Episode: 147/10000, Reward: 48.0, High score: 16, e: 0.37\n",
            "Episode: 148/10000, Reward: 96.0, High score: 16, e: 0.37\n",
            "Episode: 149/10000, Reward: 12.0, High score: 4, e: 0.37\n",
            "Episode: 150/10000, Reward: 0.0, High score: 4, e: 0.37\n",
            "Episode: 151/10000, Reward: 8.0, High score: 4, e: 0.37\n",
            "Episode: 152/10000, Reward: 80.0, High score: 16, e: 0.36\n",
            "Episode: 153/10000, Reward: 0.0, High score: 2, e: 0.36\n",
            "Episode: 154/10000, Reward: 4.0, High score: 4, e: 0.36\n",
            "Episode: 155/10000, Reward: 16.0, High score: 8, e: 0.36\n",
            "Episode: 156/10000, Reward: 0.0, High score: 4, e: 0.36\n",
            "Episode: 157/10000, Reward: 16.0, High score: 8, e: 0.36\n",
            "Episode: 158/10000, Reward: 0.0, High score: 2, e: 0.36\n",
            "Episode: 159/10000, Reward: 8.0, High score: 4, e: 0.36\n",
            "Episode: 160/10000, Reward: 0.0, High score: 2, e: 0.36\n",
            "Episode: 161/10000, Reward: 4.0, High score: 4, e: 0.36\n",
            "Episode: 162/10000, Reward: 0.0, High score: 4, e: 0.36\n",
            "Episode: 163/10000, Reward: 20.0, High score: 8, e: 0.36\n",
            "Episode: 164/10000, Reward: 88.0, High score: 16, e: 0.35\n",
            "Episode: 165/10000, Reward: 52.0, High score: 16, e: 0.35\n",
            "Episode: 166/10000, Reward: 0.0, High score: 2, e: 0.35\n",
            "Episode: 167/10000, Reward: 200.0, High score: 32, e: 0.34\n",
            "Episode: 168/10000, Reward: 8.0, High score: 4, e: 0.34\n",
            "Episode: 169/10000, Reward: 76.0, High score: 16, e: 0.34\n",
            "Episode: 170/10000, Reward: 68.0, High score: 16, e: 0.34\n",
            "Episode: 171/10000, Reward: 4.0, High score: 4, e: 0.33\n",
            "Episode: 172/10000, Reward: 4.0, High score: 4, e: 0.33\n",
            "Episode: 173/10000, Reward: 0.0, High score: 2, e: 0.33\n",
            "Episode: 174/10000, Reward: 92.0, High score: 16, e: 0.33\n",
            "Episode: 175/10000, Reward: 0.0, High score: 4, e: 0.33\n",
            "Episode: 176/10000, Reward: 16.0, High score: 8, e: 0.33\n",
            "Episode: 177/10000, Reward: 232.0, High score: 32, e: 0.32\n",
            "Episode: 178/10000, Reward: 228.0, High score: 32, e: 0.32\n",
            "Episode: 179/10000, Reward: 36.0, High score: 8, e: 0.31\n",
            "Episode: 180/10000, Reward: 4.0, High score: 4, e: 0.31\n",
            "Episode: 181/10000, Reward: 24.0, High score: 8, e: 0.31\n",
            "Episode: 182/10000, Reward: 16.0, High score: 8, e: 0.31\n",
            "Episode: 183/10000, Reward: 56.0, High score: 16, e: 0.31\n",
            "Episode: 184/10000, Reward: 4.0, High score: 4, e: 0.31\n",
            "Episode: 185/10000, Reward: 52.0, High score: 8, e: 0.31\n",
            "Episode: 186/10000, Reward: 4.0, High score: 4, e: 0.31\n",
            "Episode: 187/10000, Reward: 92.0, High score: 16, e: 0.3\n",
            "Episode: 188/10000, Reward: 104.0, High score: 16, e: 0.3\n",
            "Episode: 189/10000, Reward: 20.0, High score: 8, e: 0.3\n",
            "Episode: 190/10000, Reward: 36.0, High score: 8, e: 0.3\n",
            "Episode: 191/10000, Reward: 24.0, High score: 8, e: 0.29\n",
            "Episode: 192/10000, Reward: 8.0, High score: 4, e: 0.29\n",
            "Episode: 193/10000, Reward: 236.0, High score: 32, e: 0.29\n",
            "Episode: 194/10000, Reward: 8.0, High score: 4, e: 0.29\n",
            "Episode: 195/10000, Reward: 0.0, High score: 2, e: 0.29\n",
            "Episode: 196/10000, Reward: 4.0, High score: 4, e: 0.28\n",
            "Episode: 197/10000, Reward: 8.0, High score: 4, e: 0.28\n",
            "Episode: 198/10000, Reward: 20.0, High score: 8, e: 0.28\n",
            "Episode: 199/10000, Reward: 16.0, High score: 8, e: 0.28\n",
            "Episode: 200/10000, Reward: 196.0, High score: 32, e: 0.28\n",
            "Episode: 201/10000, Reward: 20.0, High score: 8, e: 0.28\n",
            "Episode: 202/10000, Reward: 12.0, High score: 4, e: 0.27\n",
            "Episode: 203/10000, Reward: 0.0, High score: 4, e: 0.27\n",
            "Episode: 204/10000, Reward: 40.0, High score: 8, e: 0.27\n",
            "Episode: 205/10000, Reward: 4.0, High score: 4, e: 0.27\n",
            "Episode: 206/10000, Reward: 36.0, High score: 8, e: 0.27\n",
            "Episode: 207/10000, Reward: 32.0, High score: 8, e: 0.27\n",
            "Episode: 208/10000, Reward: 24.0, High score: 8, e: 0.27\n",
            "Episode: 209/10000, Reward: 52.0, High score: 16, e: 0.27\n",
            "Episode: 210/10000, Reward: 216.0, High score: 32, e: 0.26\n",
            "Episode: 211/10000, Reward: 20.0, High score: 8, e: 0.26\n",
            "Episode: 212/10000, Reward: 92.0, High score: 16, e: 0.26\n",
            "Episode: 213/10000, Reward: 8.0, High score: 8, e: 0.26\n",
            "Episode: 214/10000, Reward: 12.0, High score: 4, e: 0.26\n",
            "Episode: 215/10000, Reward: 80.0, High score: 16, e: 0.25\n",
            "Episode: 216/10000, Reward: 40.0, High score: 8, e: 0.25\n",
            "Episode: 217/10000, Reward: 144.0, High score: 32, e: 0.25\n",
            "Episode: 218/10000, Reward: 20.0, High score: 8, e: 0.25\n",
            "Episode: 219/10000, Reward: 8.0, High score: 4, e: 0.25\n",
            "Episode: 220/10000, Reward: 4.0, High score: 4, e: 0.25\n",
            "Episode: 221/10000, Reward: 12.0, High score: 8, e: 0.25\n",
            "Episode: 222/10000, Reward: 4.0, High score: 4, e: 0.24\n",
            "Episode: 223/10000, Reward: 20.0, High score: 8, e: 0.24\n",
            "Episode: 224/10000, Reward: 4.0, High score: 4, e: 0.24\n",
            "Episode: 225/10000, Reward: 36.0, High score: 8, e: 0.24\n",
            "Episode: 226/10000, Reward: 64.0, High score: 16, e: 0.24\n",
            "Episode: 227/10000, Reward: 24.0, High score: 8, e: 0.24\n",
            "Episode: 228/10000, Reward: 20.0, High score: 8, e: 0.24\n",
            "Episode: 229/10000, Reward: 8.0, High score: 4, e: 0.24\n",
            "Episode: 230/10000, Reward: 156.0, High score: 32, e: 0.23\n",
            "Episode: 231/10000, Reward: 0.0, High score: 2, e: 0.23\n",
            "Episode: 232/10000, Reward: 4.0, High score: 4, e: 0.23\n",
            "Episode: 233/10000, Reward: 72.0, High score: 16, e: 0.23\n",
            "Episode: 234/10000, Reward: 4.0, High score: 4, e: 0.23\n",
            "Episode: 235/10000, Reward: 64.0, High score: 8, e: 0.23\n",
            "Episode: 236/10000, Reward: 32.0, High score: 8, e: 0.23\n",
            "Episode: 237/10000, Reward: 4.0, High score: 4, e: 0.23\n",
            "Episode: 238/10000, Reward: 8.0, High score: 4, e: 0.23\n",
            "Episode: 239/10000, Reward: 4.0, High score: 4, e: 0.22\n",
            "Episode: 240/10000, Reward: 36.0, High score: 8, e: 0.22\n",
            "Episode: 241/10000, Reward: 0.0, High score: 4, e: 0.22\n",
            "Episode: 242/10000, Reward: 184.0, High score: 32, e: 0.22\n",
            "Episode: 243/10000, Reward: 24.0, High score: 8, e: 0.22\n",
            "Episode: 244/10000, Reward: 196.0, High score: 32, e: 0.22\n",
            "Episode: 245/10000, Reward: 4.0, High score: 4, e: 0.22\n",
            "Episode: 246/10000, Reward: 84.0, High score: 16, e: 0.21\n",
            "Episode: 247/10000, Reward: 144.0, High score: 32, e: 0.21\n",
            "Episode: 248/10000, Reward: 80.0, High score: 16, e: 0.21\n",
            "Episode: 249/10000, Reward: 8.0, High score: 4, e: 0.21\n",
            "Episode: 250/10000, Reward: 60.0, High score: 16, e: 0.21\n",
            "Episode: 251/10000, Reward: 36.0, High score: 8, e: 0.2\n",
            "Episode: 252/10000, Reward: 164.0, High score: 32, e: 0.2\n",
            "Episode: 253/10000, Reward: 4.0, High score: 4, e: 0.2\n",
            "Episode: 254/10000, Reward: 36.0, High score: 8, e: 0.2\n",
            "Episode: 255/10000, Reward: 0.0, High score: 2, e: 0.2\n",
            "Episode: 256/10000, Reward: 72.0, High score: 16, e: 0.2\n",
            "Episode: 257/10000, Reward: 76.0, High score: 16, e: 0.2\n",
            "Episode: 258/10000, Reward: 4.0, High score: 4, e: 0.2\n",
            "Episode: 259/10000, Reward: 48.0, High score: 8, e: 0.19\n",
            "Episode: 260/10000, Reward: 20.0, High score: 8, e: 0.19\n",
            "Episode: 261/10000, Reward: 28.0, High score: 8, e: 0.19\n",
            "Episode: 262/10000, Reward: 8.0, High score: 4, e: 0.19\n",
            "Episode: 263/10000, Reward: 32.0, High score: 8, e: 0.19\n",
            "Episode: 264/10000, Reward: 32.0, High score: 8, e: 0.19\n",
            "Episode: 265/10000, Reward: 4.0, High score: 4, e: 0.19\n",
            "Episode: 266/10000, Reward: 188.0, High score: 32, e: 0.19\n",
            "Episode: 267/10000, Reward: 4.0, High score: 4, e: 0.19\n",
            "Episode: 268/10000, Reward: 20.0, High score: 8, e: 0.18\n",
            "Episode: 269/10000, Reward: 0.0, High score: 2, e: 0.18\n",
            "Episode: 270/10000, Reward: 44.0, High score: 16, e: 0.18\n",
            "Episode: 271/10000, Reward: 20.0, High score: 8, e: 0.18\n",
            "Episode: 272/10000, Reward: 32.0, High score: 8, e: 0.18\n",
            "Episode: 273/10000, Reward: 8.0, High score: 4, e: 0.18\n",
            "Episode: 274/10000, Reward: 0.0, High score: 4, e: 0.18\n",
            "Episode: 275/10000, Reward: 44.0, High score: 8, e: 0.18\n",
            "Episode: 276/10000, Reward: 68.0, High score: 16, e: 0.18\n",
            "Episode: 277/10000, Reward: 36.0, High score: 8, e: 0.18\n",
            "Episode: 278/10000, Reward: 4.0, High score: 4, e: 0.18\n",
            "Episode: 279/10000, Reward: 120.0, High score: 16, e: 0.17\n",
            "Episode: 280/10000, Reward: 4.0, High score: 4, e: 0.17\n",
            "Episode: 281/10000, Reward: 36.0, High score: 8, e: 0.17\n",
            "Episode: 282/10000, Reward: 32.0, High score: 8, e: 0.17\n",
            "Episode: 283/10000, Reward: 28.0, High score: 8, e: 0.17\n",
            "Episode: 284/10000, Reward: 8.0, High score: 4, e: 0.17\n",
            "Episode: 285/10000, Reward: 4.0, High score: 4, e: 0.17\n",
            "Episode: 286/10000, Reward: 4.0, High score: 4, e: 0.17\n",
            "Episode: 287/10000, Reward: 28.0, High score: 8, e: 0.17\n",
            "Episode: 288/10000, Reward: 4.0, High score: 4, e: 0.17\n",
            "Episode: 289/10000, Reward: 4.0, High score: 4, e: 0.17\n",
            "Episode: 290/10000, Reward: 20.0, High score: 8, e: 0.17\n",
            "Episode: 291/10000, Reward: 24.0, High score: 8, e: 0.17\n",
            "Episode: 292/10000, Reward: 140.0, High score: 16, e: 0.16\n",
            "Episode: 293/10000, Reward: 32.0, High score: 8, e: 0.16\n",
            "Episode: 294/10000, Reward: 52.0, High score: 8, e: 0.16\n",
            "Episode: 295/10000, Reward: 8.0, High score: 4, e: 0.16\n",
            "Episode: 296/10000, Reward: 96.0, High score: 16, e: 0.16\n",
            "Episode: 297/10000, Reward: 0.0, High score: 2, e: 0.16\n",
            "Episode: 298/10000, Reward: 0.0, High score: 2, e: 0.16\n",
            "Episode: 299/10000, Reward: 84.0, High score: 16, e: 0.16\n",
            "Episode: 300/10000, Reward: 60.0, High score: 16, e: 0.16\n",
            "Episode: 301/10000, Reward: 8.0, High score: 4, e: 0.16\n",
            "Episode: 302/10000, Reward: 20.0, High score: 8, e: 0.15\n",
            "Episode: 303/10000, Reward: 0.0, High score: 4, e: 0.15\n",
            "Episode: 304/10000, Reward: 124.0, High score: 16, e: 0.15\n",
            "Episode: 305/10000, Reward: 104.0, High score: 16, e: 0.15\n",
            "Episode: 306/10000, Reward: 0.0, High score: 2, e: 0.15\n",
            "Episode: 307/10000, Reward: 20.0, High score: 8, e: 0.15\n",
            "Episode: 308/10000, Reward: 64.0, High score: 8, e: 0.15\n",
            "Episode: 309/10000, Reward: 0.0, High score: 4, e: 0.15\n",
            "Episode: 310/10000, Reward: 4.0, High score: 4, e: 0.15\n",
            "Episode: 311/10000, Reward: 116.0, High score: 16, e: 0.15\n",
            "Episode: 312/10000, Reward: 0.0, High score: 4, e: 0.15\n",
            "Episode: 313/10000, Reward: 4.0, High score: 4, e: 0.15\n",
            "Episode: 314/10000, Reward: 20.0, High score: 8, e: 0.15\n",
            "Episode: 315/10000, Reward: 80.0, High score: 16, e: 0.14\n",
            "Episode: 316/10000, Reward: 0.0, High score: 4, e: 0.14\n",
            "Episode: 317/10000, Reward: 24.0, High score: 8, e: 0.14\n",
            "Episode: 318/10000, Reward: 0.0, High score: 2, e: 0.14\n",
            "Episode: 319/10000, Reward: 44.0, High score: 8, e: 0.14\n",
            "Episode: 320/10000, Reward: 52.0, High score: 16, e: 0.14\n",
            "Episode: 321/10000, Reward: 20.0, High score: 8, e: 0.14\n",
            "Episode: 322/10000, Reward: 92.0, High score: 16, e: 0.14\n",
            "Episode: 323/10000, Reward: 36.0, High score: 8, e: 0.14\n",
            "Episode: 324/10000, Reward: 216.0, High score: 32, e: 0.14\n",
            "Episode: 325/10000, Reward: 60.0, High score: 16, e: 0.13\n",
            "Episode: 326/10000, Reward: 8.0, High score: 4, e: 0.13\n",
            "Episode: 327/10000, Reward: 156.0, High score: 16, e: 0.13\n",
            "Episode: 328/10000, Reward: 60.0, High score: 16, e: 0.13\n",
            "Episode: 329/10000, Reward: 148.0, High score: 32, e: 0.13\n",
            "Episode: 330/10000, Reward: 24.0, High score: 8, e: 0.13\n",
            "Episode: 331/10000, Reward: 20.0, High score: 8, e: 0.13\n",
            "Episode: 332/10000, Reward: 28.0, High score: 8, e: 0.13\n",
            "Episode: 333/10000, Reward: 216.0, High score: 32, e: 0.13\n",
            "Episode: 334/10000, Reward: 36.0, High score: 8, e: 0.12\n",
            "Episode: 335/10000, Reward: 20.0, High score: 8, e: 0.12\n",
            "Episode: 336/10000, Reward: 24.0, High score: 8, e: 0.12\n",
            "Episode: 337/10000, Reward: 8.0, High score: 4, e: 0.12\n",
            "Episode: 338/10000, Reward: 16.0, High score: 8, e: 0.12\n",
            "Episode: 339/10000, Reward: 8.0, High score: 4, e: 0.12\n",
            "Episode: 340/10000, Reward: 24.0, High score: 8, e: 0.12\n",
            "Episode: 341/10000, Reward: 4.0, High score: 4, e: 0.12\n",
            "Episode: 342/10000, Reward: 172.0, High score: 32, e: 0.12\n",
            "Episode: 343/10000, Reward: 108.0, High score: 16, e: 0.12\n",
            "Episode: 344/10000, Reward: 4.0, High score: 4, e: 0.12\n",
            "Episode: 345/10000, Reward: 60.0, High score: 16, e: 0.12\n",
            "Episode: 346/10000, Reward: 32.0, High score: 8, e: 0.12\n",
            "Episode: 347/10000, Reward: 56.0, High score: 16, e: 0.12\n",
            "Episode: 348/10000, Reward: 20.0, High score: 8, e: 0.12\n",
            "Episode: 349/10000, Reward: 0.0, High score: 4, e: 0.12\n",
            "Episode: 350/10000, Reward: 152.0, High score: 32, e: 0.11\n",
            "Episode: 351/10000, Reward: 56.0, High score: 16, e: 0.11\n",
            "Episode: 352/10000, Reward: 104.0, High score: 16, e: 0.11\n",
            "Episode: 353/10000, Reward: 0.0, High score: 4, e: 0.11\n",
            "Episode: 354/10000, Reward: 60.0, High score: 8, e: 0.11\n",
            "Episode: 355/10000, Reward: 20.0, High score: 8, e: 0.11\n",
            "Episode: 356/10000, Reward: 8.0, High score: 4, e: 0.11\n",
            "Episode: 357/10000, Reward: 84.0, High score: 16, e: 0.11\n",
            "Episode: 358/10000, Reward: 4.0, High score: 4, e: 0.11\n",
            "Episode: 359/10000, Reward: 0.0, High score: 2, e: 0.11\n",
            "Episode: 360/10000, Reward: 64.0, High score: 16, e: 0.11\n",
            "Episode: 361/10000, Reward: 16.0, High score: 8, e: 0.11\n",
            "Episode: 362/10000, Reward: 8.0, High score: 4, e: 0.11\n",
            "Episode: 363/10000, Reward: 24.0, High score: 8, e: 0.11\n",
            "Episode: 364/10000, Reward: 104.0, High score: 16, e: 0.11\n",
            "Episode: 365/10000, Reward: 80.0, High score: 16, e: 0.1\n",
            "Episode: 366/10000, Reward: 16.0, High score: 8, e: 0.1\n",
            "Episode: 367/10000, Reward: 0.0, High score: 4, e: 0.1\n",
            "Episode: 368/10000, Reward: 48.0, High score: 16, e: 0.1\n",
            "Episode: 369/10000, Reward: 4.0, High score: 4, e: 0.1\n",
            "Episode: 370/10000, Reward: 28.0, High score: 8, e: 0.1\n",
            "Episode: 371/10000, Reward: 24.0, High score: 8, e: 0.1\n",
            "Episode: 372/10000, Reward: 24.0, High score: 8, e: 0.1\n",
            "Episode: 373/10000, Reward: 0.0, High score: 2, e: 0.1\n",
            "Episode: 374/10000, Reward: 16.0, High score: 8, e: 0.1\n",
            "Episode: 375/10000, Reward: 88.0, High score: 16, e: 0.1\n",
            "Episode: 376/10000, Reward: 100.0, High score: 16, e: 0.099\n",
            "Episode: 377/10000, Reward: 20.0, High score: 8, e: 0.098\n",
            "Episode: 378/10000, Reward: 0.0, High score: 2, e: 0.098\n",
            "Episode: 379/10000, Reward: 40.0, High score: 8, e: 0.097\n",
            "Episode: 380/10000, Reward: 8.0, High score: 4, e: 0.097\n",
            "Episode: 381/10000, Reward: 4.0, High score: 4, e: 0.097\n",
            "Episode: 382/10000, Reward: 0.0, High score: 4, e: 0.097\n",
            "Episode: 383/10000, Reward: 36.0, High score: 8, e: 0.096\n",
            "Episode: 384/10000, Reward: 32.0, High score: 8, e: 0.096\n",
            "Episode: 385/10000, Reward: 16.0, High score: 8, e: 0.095\n",
            "Episode: 386/10000, Reward: 16.0, High score: 8, e: 0.095\n",
            "Episode: 387/10000, Reward: 100.0, High score: 16, e: 0.094\n",
            "Episode: 388/10000, Reward: 32.0, High score: 8, e: 0.093\n",
            "Episode: 389/10000, Reward: 212.0, High score: 32, e: 0.091\n",
            "Episode: 390/10000, Reward: 20.0, High score: 8, e: 0.091\n",
            "Episode: 391/10000, Reward: 8.0, High score: 4, e: 0.091\n",
            "Episode: 392/10000, Reward: 8.0, High score: 4, e: 0.09\n",
            "Episode: 393/10000, Reward: 84.0, High score: 16, e: 0.09\n",
            "Episode: 394/10000, Reward: 20.0, High score: 8, e: 0.089\n",
            "Episode: 395/10000, Reward: 12.0, High score: 4, e: 0.089\n",
            "Episode: 396/10000, Reward: 4.0, High score: 4, e: 0.089\n",
            "Episode: 397/10000, Reward: 24.0, High score: 8, e: 0.088\n",
            "Episode: 398/10000, Reward: 4.0, High score: 4, e: 0.088\n",
            "Episode: 399/10000, Reward: 20.0, High score: 8, e: 0.088\n",
            "Episode: 400/10000, Reward: 76.0, High score: 16, e: 0.087\n",
            "Episode: 401/10000, Reward: 12.0, High score: 8, e: 0.087\n",
            "Episode: 402/10000, Reward: 16.0, High score: 8, e: 0.086\n",
            "Episode: 403/10000, Reward: 120.0, High score: 16, e: 0.085\n",
            "Episode: 404/10000, Reward: 0.0, High score: 2, e: 0.085\n",
            "Episode: 405/10000, Reward: 4.0, High score: 4, e: 0.085\n",
            "Episode: 406/10000, Reward: 52.0, High score: 16, e: 0.084\n",
            "Episode: 407/10000, Reward: 8.0, High score: 4, e: 0.084\n",
            "Episode: 408/10000, Reward: 32.0, High score: 8, e: 0.084\n",
            "Episode: 409/10000, Reward: 32.0, High score: 8, e: 0.083\n",
            "Episode: 410/10000, Reward: 40.0, High score: 8, e: 0.083\n",
            "Episode: 411/10000, Reward: 24.0, High score: 8, e: 0.082\n",
            "Episode: 412/10000, Reward: 4.0, High score: 4, e: 0.082\n",
            "Episode: 413/10000, Reward: 12.0, High score: 4, e: 0.082\n",
            "Episode: 414/10000, Reward: 20.0, High score: 8, e: 0.081\n",
            "Episode: 415/10000, Reward: 60.0, High score: 16, e: 0.081\n",
            "Episode: 416/10000, Reward: 0.0, High score: 4, e: 0.081\n",
            "Episode: 417/10000, Reward: 40.0, High score: 8, e: 0.08\n",
            "Episode: 418/10000, Reward: 44.0, High score: 8, e: 0.079\n",
            "Episode: 419/10000, Reward: 52.0, High score: 16, e: 0.079\n",
            "Episode: 420/10000, Reward: 20.0, High score: 8, e: 0.079\n",
            "Episode: 421/10000, Reward: 0.0, High score: 2, e: 0.079\n",
            "Episode: 422/10000, Reward: 64.0, High score: 16, e: 0.078\n",
            "Episode: 423/10000, Reward: 28.0, High score: 8, e: 0.077\n",
            "Episode: 424/10000, Reward: 40.0, High score: 8, e: 0.077\n",
            "Episode: 425/10000, Reward: 0.0, High score: 4, e: 0.077\n",
            "Episode: 426/10000, Reward: 8.0, High score: 4, e: 0.077\n",
            "Episode: 427/10000, Reward: 4.0, High score: 4, e: 0.076\n",
            "Episode: 428/10000, Reward: 36.0, High score: 8, e: 0.076\n",
            "Episode: 429/10000, Reward: 108.0, High score: 16, e: 0.075\n",
            "Episode: 430/10000, Reward: 88.0, High score: 16, e: 0.074\n",
            "Episode: 431/10000, Reward: 8.0, High score: 4, e: 0.074\n",
            "Episode: 432/10000, Reward: 4.0, High score: 4, e: 0.074\n",
            "Episode: 433/10000, Reward: 32.0, High score: 8, e: 0.073\n",
            "Episode: 434/10000, Reward: 16.0, High score: 8, e: 0.073\n",
            "Episode: 435/10000, Reward: 64.0, High score: 16, e: 0.072\n",
            "Episode: 436/10000, Reward: 4.0, High score: 4, e: 0.072\n",
            "Episode: 437/10000, Reward: 4.0, High score: 4, e: 0.072\n",
            "Episode: 438/10000, Reward: 16.0, High score: 8, e: 0.072\n",
            "Episode: 439/10000, Reward: 56.0, High score: 16, e: 0.071\n",
            "Episode: 440/10000, Reward: 88.0, High score: 16, e: 0.071\n",
            "Episode: 441/10000, Reward: 52.0, High score: 16, e: 0.07\n",
            "Episode: 442/10000, Reward: 24.0, High score: 8, e: 0.07\n",
            "Episode: 443/10000, Reward: 16.0, High score: 8, e: 0.07\n",
            "Episode: 444/10000, Reward: 56.0, High score: 16, e: 0.069\n",
            "Episode: 445/10000, Reward: 20.0, High score: 8, e: 0.069\n",
            "Episode: 446/10000, Reward: 28.0, High score: 8, e: 0.068\n",
            "Episode: 447/10000, Reward: 48.0, High score: 16, e: 0.068\n",
            "Episode: 448/10000, Reward: 28.0, High score: 8, e: 0.067\n",
            "Episode: 449/10000, Reward: 0.0, High score: 2, e: 0.067\n",
            "Episode: 450/10000, Reward: 36.0, High score: 8, e: 0.067\n",
            "Episode: 451/10000, Reward: 16.0, High score: 8, e: 0.067\n",
            "Episode: 452/10000, Reward: 72.0, High score: 16, e: 0.066\n",
            "Episode: 453/10000, Reward: 64.0, High score: 16, e: 0.066\n",
            "Episode: 454/10000, Reward: 0.0, High score: 4, e: 0.066\n",
            "Episode: 455/10000, Reward: 20.0, High score: 8, e: 0.065\n",
            "Episode: 456/10000, Reward: 24.0, High score: 8, e: 0.065\n",
            "Episode: 457/10000, Reward: 8.0, High score: 4, e: 0.065\n",
            "Episode: 458/10000, Reward: 28.0, High score: 8, e: 0.065\n",
            "Episode: 459/10000, Reward: 28.0, High score: 8, e: 0.064\n",
            "Episode: 460/10000, Reward: 12.0, High score: 4, e: 0.064\n",
            "Episode: 461/10000, Reward: 36.0, High score: 8, e: 0.064\n",
            "Episode: 462/10000, Reward: 0.0, High score: 4, e: 0.064\n",
            "Episode: 463/10000, Reward: 28.0, High score: 8, e: 0.063\n",
            "Episode: 464/10000, Reward: 0.0, High score: 4, e: 0.063\n",
            "Episode: 465/10000, Reward: 16.0, High score: 8, e: 0.063\n",
            "Episode: 466/10000, Reward: 20.0, High score: 8, e: 0.063\n",
            "Episode: 467/10000, Reward: 92.0, High score: 16, e: 0.062\n",
            "Episode: 468/10000, Reward: 68.0, High score: 16, e: 0.061\n",
            "Episode: 469/10000, Reward: 8.0, High score: 4, e: 0.061\n",
            "Episode: 470/10000, Reward: 0.0, High score: 4, e: 0.061\n",
            "Episode: 471/10000, Reward: 24.0, High score: 8, e: 0.061\n",
            "Episode: 472/10000, Reward: 60.0, High score: 16, e: 0.06\n",
            "Episode: 473/10000, Reward: 32.0, High score: 8, e: 0.06\n",
            "Episode: 474/10000, Reward: 64.0, High score: 16, e: 0.059\n",
            "Episode: 475/10000, Reward: 56.0, High score: 16, e: 0.059\n",
            "Episode: 476/10000, Reward: 20.0, High score: 8, e: 0.059\n",
            "Episode: 477/10000, Reward: 0.0, High score: 4, e: 0.059\n",
            "Episode: 478/10000, Reward: 116.0, High score: 16, e: 0.058\n",
            "Episode: 479/10000, Reward: 20.0, High score: 8, e: 0.058\n",
            "Episode: 480/10000, Reward: 4.0, High score: 4, e: 0.057\n",
            "Episode: 481/10000, Reward: 24.0, High score: 8, e: 0.057\n",
            "Episode: 482/10000, Reward: 16.0, High score: 8, e: 0.057\n",
            "Episode: 483/10000, Reward: 24.0, High score: 8, e: 0.057\n",
            "Episode: 484/10000, Reward: 40.0, High score: 8, e: 0.056\n",
            "Episode: 485/10000, Reward: 12.0, High score: 4, e: 0.056\n",
            "Episode: 486/10000, Reward: 0.0, High score: 2, e: 0.056\n",
            "Episode: 487/10000, Reward: 20.0, High score: 8, e: 0.056\n",
            "Episode: 488/10000, Reward: 24.0, High score: 8, e: 0.055\n",
            "Episode: 489/10000, Reward: 0.0, High score: 2, e: 0.055\n",
            "Episode: 490/10000, Reward: 16.0, High score: 8, e: 0.055\n",
            "Episode: 491/10000, Reward: 96.0, High score: 16, e: 0.055\n",
            "Episode: 492/10000, Reward: 72.0, High score: 16, e: 0.054\n",
            "Episode: 493/10000, Reward: 24.0, High score: 8, e: 0.054\n",
            "Episode: 494/10000, Reward: 108.0, High score: 16, e: 0.053\n",
            "Episode: 495/10000, Reward: 32.0, High score: 8, e: 0.053\n",
            "Episode: 496/10000, Reward: 48.0, High score: 16, e: 0.053\n",
            "Episode: 497/10000, Reward: 68.0, High score: 16, e: 0.052\n",
            "Episode: 498/10000, Reward: 0.0, High score: 4, e: 0.052\n",
            "Episode: 499/10000, Reward: 4.0, High score: 4, e: 0.052\n",
            "Episode: 500/10000, Reward: 8.0, High score: 4, e: 0.052\n",
            "Episode: 501/10000, Reward: 4.0, High score: 4, e: 0.052\n",
            "Episode: 502/10000, Reward: 60.0, High score: 8, e: 0.051\n",
            "Episode: 503/10000, Reward: 4.0, High score: 4, e: 0.051\n",
            "Episode: 504/10000, Reward: 8.0, High score: 4, e: 0.051\n",
            "Episode: 505/10000, Reward: 44.0, High score: 8, e: 0.05\n",
            "Episode: 506/10000, Reward: 44.0, High score: 16, e: 0.05\n",
            "Episode: 507/10000, Reward: 20.0, High score: 8, e: 0.05\n",
            "Episode: 508/10000, Reward: 16.0, High score: 8, e: 0.05\n",
            "Episode: 509/10000, Reward: 64.0, High score: 16, e: 0.049\n",
            "Episode: 510/10000, Reward: 128.0, High score: 16, e: 0.049\n",
            "Episode: 511/10000, Reward: 8.0, High score: 4, e: 0.049\n",
            "Episode: 512/10000, Reward: 20.0, High score: 8, e: 0.048\n",
            "Episode: 513/10000, Reward: 0.0, High score: 4, e: 0.048\n",
            "Episode: 514/10000, Reward: 0.0, High score: 2, e: 0.048\n",
            "Episode: 515/10000, Reward: 36.0, High score: 8, e: 0.048\n",
            "Episode: 516/10000, Reward: 72.0, High score: 16, e: 0.048\n",
            "Episode: 517/10000, Reward: 16.0, High score: 8, e: 0.047\n",
            "Episode: 518/10000, Reward: 20.0, High score: 8, e: 0.047\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0bd1ee8e318d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_s_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m       \u001b[0mp_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2027\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2028\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2029\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2030\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    719\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3410\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEEP Q-LEARNING v1.0\n",
        "# Parameters\n",
        "episodes = 10000\n",
        "discount = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_max = 1.0\n",
        "epsilon_decrement = epsilon_max - epsilon_min / (episodes // 4)\n",
        "batch_size = 32  # Batch size to be taken from replay memory\n",
        "# Replay memory, stores s, a, r, s'\n",
        "replay_s = []\n",
        "replay_a = []\n",
        "replay_rewards = []\n",
        "replay_s_prime = []\n",
        "max_memory_length = 5000  # Max size of replay memory\n",
        "update_t_network = 1000  # Number of time steps before updating target network\n",
        "ep_reward_history = [0]\n",
        "high_score = []\n",
        "total_steps = 0\n",
        "\n",
        "num_actions = 4  # Number of possible actions\n",
        "input_shape = (4, 4, 16)\n",
        "p_network = DeepQ(input_shape)  # Initialize policy network\n",
        "p_network.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "t_network = DeepQ(input_shape)  # Initialize target network\n",
        "t_network.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # Optimizer\n",
        "loss_fn = keras.losses.MeanSquaredError()  # Loss function\n",
        "\n",
        "# Train\n",
        "for ep in range(episodes):\n",
        "  s = np.array(env.reset())\n",
        "  ep_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    total_steps += 1\n",
        "    # Select action with e-greedy\n",
        "    if np.random.random() < epsilon:  # Random choice\n",
        "      a = np.random.choice(num_actions)\n",
        "    else:  # Greedy choice\n",
        "      s_tensor = tf.convert_to_tensor(s)\n",
        "      s_tensor = tf.expand_dims(s_tensor, 0)\n",
        "      a = tf.argmax(p_network(s_tensor, training=False)[0]).numpy()\n",
        "    \n",
        "    epsilon = max(epsilon - epsilon_decrement, epsilon_min)  # Epsilon decay\n",
        "    \n",
        "    # Execute action and observe s', reward, and done\n",
        "    s_prime, reward, done, _ = env.step(a)\n",
        "    s_prime = np.array(s_prime)\n",
        "    ep_reward += reward\n",
        "    \n",
        "    # Store experience in replay memory up to the last N number of experiences\n",
        "    replay_s.append(s)\n",
        "    replay_a.append(a)\n",
        "    replay_rewards.append(reward)\n",
        "    replay_s_prime.append(s_prime)\n",
        "    s = s_prime\n",
        "    if len(replay_s) > max_memory_length:\n",
        "      del replay_s[:1]\n",
        "      del replay_a[:1]\n",
        "      del replay_rewards[:1]\n",
        "      del replay_s_prime[:1]\n",
        "    \n",
        "    if len(replay_s) > batch_size:\n",
        "      # Sample random batch from replay memory\n",
        "      indices = np.random.choice(range(len(replay_s)), size=batch_size)\n",
        "      batch_s = np.array([replay_s[i] for i in indices])\n",
        "      batch_a = np.array([replay_a[i] for i in indices])\n",
        "      batch_rewards = np.array([replay_rewards[i] for i in indices])\n",
        "      batch_s_prime = np.array([replay_s_prime[i] for i in indices])\n",
        "\n",
        "      # Pass s_primes of batch to target network\n",
        "      q_values_prime = t_network.predict(batch_s_prime)\n",
        "      # q_values_prime = batch_rewards + discount * tf.reduce_max(q_values_prime, axis=1)\n",
        "      q_values_prime = batch_rewards + discount * tf.argmax(q_values_prime)\n",
        "\n",
        "      p_network.fit(batch_s, q_values_prime)\n",
        "\n",
        "      masks = tf.one_hot(batch_a, num_actions)\n",
        "      with tf.GradientTape() as tape:\n",
        "        # Pass batch to policy network\n",
        "        q_values = p_network(batch_s)\n",
        "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        # Calculate loss between output and target q-values\n",
        "        loss = loss_fn(q_values_prime, q_action)\n",
        "      # Update weights in policy network to minimize loss, backpropagation\n",
        "      grads = tape.gradient(loss, p_network.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, p_network.trainable_variables))\n",
        "    \n",
        "    # After x time steps, update target network with policy network weights\n",
        "    if total_steps % update_t_network == 0:\n",
        "      t_network.set_weights(p_network.get_weights())\n",
        "\n",
        "    # Track highest score reach by any episode\n",
        "    if done:\n",
        "      print(s)\n",
        "      high_score.append(max(s))\n",
        "      if ep % 500 == 0:\n",
        "        print(high_score[-1])\n",
        "\n",
        "  # Track reward achieved by each episode\n",
        "  ep_reward_history.append(ep_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "Jfqal66hPm6g",
        "outputId": "152d2311-e0af-4bbc-adde-2ca01a1bbcfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f072a018c719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0mhigh_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    }
  ]
}